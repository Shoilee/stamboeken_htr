{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408ce71a",
   "metadata": {},
   "source": [
    "First, activate conda environment:\n",
    "```bash\n",
    "conda activate NGTR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Go outside the src directory\n",
    "os.chdir(\"../Image2Table_LLM\")\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current directory:\", current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f944d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/images\"\n",
    "GT_HTML_DIR = \"../data/labels/tables\"\n",
    "GT_INFO_DIR = \"../data/labels/info\"\n",
    "HTML_DIR = \"../data/tables/html\"\n",
    "OUTPUT_JSON_DIR = \"../data/json\"\n",
    "TEMP_DIR = \"../data/temp\"\n",
    "SCHEMA_PATH = \"../data/schema/personbasicinfo.yaml\"\n",
    "LLM_MODEL = \"ollama/llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM import call_LLM\n",
    "from LLM_key import llm_model\n",
    "from parse import extract_HTML, format_td\n",
    "from prompt import tsr_html_prompt, tsr_html_decomposition\n",
    "from metric import TEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_html_table_for_single_image(image_name, output_html_path=HTML_DIR):\n",
    "    image_path = os.path.join(DATA_DIR, image_name)\n",
    "    llm_html = call_LLM(image_path, prompt=tsr_html_prompt, model_name=llm_model, temperature=0)\n",
    "    llm_html = extract_HTML(llm_html)\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    llm_html = llm_html.replace(\"<table>\", \"<table border='1'>\")\n",
    "    soup = BeautifulSoup(llm_html, 'html.parser')\n",
    "    with open(os.path.join(output_html_path, image_name+'.html'), 'w', encoding='utf-8') as f:\n",
    "        f.write(soup.prettify())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for file in os.listdir(DATA_DIR): \n",
    "        if not file.endswith(\".jpg\"): \n",
    "            continue \n",
    "\n",
    "        image_name = file\n",
    "        build_html_table_for_single_image(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b947683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM import call_LLM\n",
    "from LLM_key import llm_model\n",
    "from parse import extract_HTML, format_td\n",
    "from prompt import tsr_html_prompt, tsr_html_decomposition\n",
    "from metric import TEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a382d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/images/NL-HaNA_2.10.50_45_0110.jpg\"\n",
    "image_name = os.path.basename(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f380809",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_html = call_LLM(image_path, prompt=tsr_html_prompt, model_name=llm_model, temperature=0)\n",
    "llm_html = extract_HTML(llm_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "llm_html = llm_html.replace(\"<table>\", \"<table border='1'>\")\n",
    "soup = BeautifulSoup(llm_html, 'html.parser')\n",
    "with open(os.path.join(\"../data/tables/html\", image_name+'.html'), 'w', encoding='utf-8') as f:\n",
    "    f.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6705edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TEDS(ground_truth_html, predicted_html):\n",
    "    predicted_html = format_td(predicted_html)\n",
    "    ground_truth_html = format_td(ground_truth_html)\n",
    "\n",
    "    teds = TEDS(structure_only=False)\n",
    "    teds_score = teds.evaluate(ground_truth_html, predicted_html)\n",
    "\n",
    "    teds_struct = TEDS(structure_only=True)\n",
    "    teds_struct_score = teds_struct.evaluate(ground_truth_html, predicted_html)\n",
    "    \n",
    "    print(f\"TEDS: {teds_score:.4f}\")\n",
    "    print(f\"TEDS-Struct: {teds_struct_score:.4f}\")\n",
    "\n",
    "    return teds_score, teds_struct_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"../data/labels/tables\", image_name+'.html'), 'r', encoding='utf-8') as f:\n",
    "    label_html = f.read()\n",
    "\n",
    "calculate_TEDS(label_html, llm_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a032de7",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go outside the src directory\n",
    "os.chdir(\"..\")\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current directory:\", current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16115c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(llm_html, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "# --- Step 1: make a grid keeping track of rowspans ---\n",
    "logical_rows = []\n",
    "rowspans = {}  # {(col_idx): [remaining_rows, cell_content]}\n",
    "\n",
    "for r_idx, tr in enumerate(rows):\n",
    "    # Start with any carried over rowspans from previous rows\n",
    "    current_row = []\n",
    "    to_remove = []\n",
    "\n",
    "    # Fill carried-down cells first\n",
    "    for col_idx, (remaining, cell) in rowspans.items():\n",
    "        current_row.append(cell)\n",
    "        rowspans[col_idx][0] -= 1\n",
    "        if rowspans[col_idx][0] <= 0:\n",
    "            to_remove.append(col_idx)\n",
    "    for col_idx in to_remove:\n",
    "        del rowspans[col_idx]\n",
    "\n",
    "    # Now add new cells from this row\n",
    "    c_idx = 0\n",
    "    for td in tr.find_all('td'):\n",
    "        while any(k == c_idx for k in rowspans):  # skip columns occupied by carried cells\n",
    "            c_idx += 1\n",
    "        text = td.get_text(\" \", strip=True)\n",
    "        cell_id = td.get('id')\n",
    "        r_idx = int(td.get('row', r_idx))\n",
    "        c_idx = int(td.get('col', c_idx))\n",
    "        rowspan = int(td.get('rowspan', 1))\n",
    "        colspan = int(td.get('colspan', 1))\n",
    "\n",
    "        cell_data = {\n",
    "            'text': text,\n",
    "            'id': cell_id,\n",
    "            'row': r_idx,\n",
    "            'col': c_idx,\n",
    "            'rowspan': rowspan,\n",
    "            'colspan': colspan\n",
    "        }\n",
    "\n",
    "        # Place cell in current row\n",
    "        current_row.append(cell_data)\n",
    "\n",
    "        # Store for future rows if rowspan > 1\n",
    "        if rowspan > 1:\n",
    "            rowspans[c_idx] = [rowspan - 1, cell_data]\n",
    "\n",
    "        c_idx += colspan\n",
    "\n",
    "    logical_rows.append(current_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pagexml_path = \"../data/tables/pagexml/NL-HaNA_2.10.50_45_0110.jpg.xml\"\n",
    "    image_path = \"../data/images/NL-HaNA_2.10.50_45_0110.jpg\"\n",
    "    output_image_path = \"../data/tables/visualisations/NL-HaNA_2.10.50_45_0110_viz.jpg\"\n",
    "\n",
    "    visualizer = PageXMLVisualizer(pagexml_path, image_path)\n",
    "    visualizer.parse_pagexml()\n",
    "    annotated_image = visualizer.draw_visualisation()\n",
    "\n",
    "    # Save the visualised image\n",
    "    cv2.imwrite(output_image_path, annotated_image)\n",
    "    print(f\"Visualisation saved to {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d921ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from src.person_info_extraction import extract_info_regex, extract_info_LLM\n",
    "\n",
    "persons = []\n",
    "\n",
    "for i, row in enumerate(logical_rows):\n",
    "    person = {}\n",
    "        \n",
    "    # print(generate_prompt(cells))\n",
    "    person = json.loads(extract_info_LLM(row))\n",
    "    if all(v['value']==None for v in person.values()):\n",
    "        continue\n",
    "\n",
    "    if person:\n",
    "        persons.append(person)\n",
    "\n",
    "unique_persons = {json.dumps(person, sort_keys=True) for person in persons}\n",
    "unique_persons_list = [json.loads(p) for p in unique_persons]\n",
    "        \n",
    "json_obj = {\"persons\": unique_persons_list}\n",
    "print(json.dumps(json_obj, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/json/{image_name}.json\", \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(json_obj, json_file, ensure_ascii=False, indent=2)#!/usr/bin/env python3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import best_match_similarity\n",
    "# from src.metrics import calculate_normalized_information_distance\n",
    "\n",
    "with open(f\"data/json/{image_name}.json\", 'r', encoding='utf-8') as f:\n",
    "    constructed_html = json.load(f)\n",
    "\n",
    "with open(os.path.join(\"data/labels\", image_name.replace('.jpg', '.json')), 'r', encoding='utf-8') as f:\n",
    "    label_html = json.load(f)\n",
    "\n",
    "# calculate_normalized_information_distance(constructed_html, label_html)\n",
    "print(best_match_similarity(label_html.get(\"persons\", []), constructed_html.get(\"persons\", [])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a942f",
   "metadata": {},
   "source": [
    "### KG construction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json from file\n",
    "with open(f\"../data/json/{image_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_obj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT ASSERSION TRIPLES\n",
    "from rdflib import Graph, ConjunctiveGraph, Namespace, URIRef, Literal, RDF\n",
    "\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "EX = Namespace(\"http://example.org/ontology/\")\n",
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "cg = ConjunctiveGraph()\n",
    "cg.bind(\"foaf\", FOAF)\n",
    "cg.bind(\"ex\", EX)\n",
    "cg.bind(\"prov\", PROV)\n",
    "\n",
    "# Mapping from json keys to RDF predicates\n",
    "predicate_map = {\n",
    "    \"vader\": EX.vader,\n",
    "    \"moeder\": EX.moeder,\n",
    "    \"geboorte_datum\": EX.geboorteDatum,\n",
    "    \"geboorte_plaats\": EX.geboortePlaats,\n",
    "    \"laatste_woonplaats\": EX.laatsteWoonplaats\n",
    "}\n",
    "\n",
    "for idx, person in enumerate(json_obj[\"persons\"], start=1):\n",
    "    person_uri = URIRef(f\"http://example.org/person/{idx}\")\n",
    "    assertion_graph_uri = URIRef(\"http://example.org/assertion\")\n",
    "    assertion_graph = Graph(store=cg.store, identifier=assertion_graph_uri)\n",
    "    assertion_graph.add((person_uri, RDF.type, FOAF.Person))\n",
    "\n",
    "    provenance_graph_uri = URIRef(\"http://example.org/provenance\")\n",
    "    provenance_graph = Graph(store=cg.store, identifier=provenance_graph_uri)\n",
    "\n",
    "    for key, value_dict in person.items():\n",
    "        value = value_dict[\"value\"]\n",
    "        cell_id = value_dict[\"cell\"]\n",
    "        \n",
    "        predicate = predicate_map.get(key)\n",
    "        \n",
    "        # if cell_id is null, no named graph can be created\n",
    "        if not cell_id:\n",
    "            assertion_graph.add((person_uri, predicate, Literal(value)))\n",
    "            continue\n",
    "        \n",
    "        if predicate:\n",
    "            # Named graph for each cell\n",
    "            graph_uri = URIRef(f\"http://example.org/graph/{cell_id}\")\n",
    "            ng = Graph(store=cg.store, identifier=graph_uri)\n",
    "            ng.add((person_uri, predicate, Literal(value)))\n",
    "            \n",
    "\n",
    "cg.serialize(f\"../data/triples/{image_name}_assersion.trig\", format='trig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGTR",
   "language": "python",
   "name": "ngtr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
