{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9300db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb06f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/labels/\"\n",
    "image_name = \"NL-HaNA_2.10.50_45_0355.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221a9e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/sarah_shoilee/codeProjects/stamboekn_KE\n"
     ]
    }
   ],
   "source": [
    "# Go outside the src directory\n",
    "os.chdir(\"..\")\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current directory:\", current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668e6df",
   "metadata": {},
   "source": [
    "need to construct HTML Table form transkribus XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2791e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Helper Functions --------\n",
    "def parse_points(points_str):\n",
    "    \"\"\"Parse 'x,y x,y ...' string to a list of (x,y) tuples\"\"\"\n",
    "    return [tuple(map(int, p.split(','))) for p in points_str.strip().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ca8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Main Script --------\n",
    "# Load XML\n",
    "xml_path = os.path.join(directory, image_name)\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Namespace (needed because PageXML uses namespaces)\n",
    "ns = {'pc': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
    "\n",
    "# Extract table cells\n",
    "cells = {}  # cell_id -> Polygon\n",
    "for cell in root.findall('.//pc:TableCell', ns):\n",
    "    cell_id = cell.attrib['id']\n",
    "    coords = cell.find('pc:Coords', ns).attrib['points']\n",
    "    cells[cell_id] = Polygon(parse_points(coords))\n",
    "\n",
    "# Extract text lines\n",
    "text_lines = []  # list of dicts {id, polygon, text}\n",
    "for textline in root.findall('.//pc:TextLine', ns):\n",
    "    tl_id = textline.attrib['id']\n",
    "    coords = textline.find('pc:Coords', ns).attrib['points']\n",
    "    text = textline.find('.//pc:Unicode', ns)\n",
    "    text_value = text.text if text is not None else ''\n",
    "    text_lines.append({\n",
    "        'id': tl_id,\n",
    "        'polygon': Polygon(parse_points(coords)),\n",
    "        'text': text_value\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747e41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell c_44 combined text (reversed):\n",
      "  Hop\n",
      "Cell c_44 combined text:\n",
      "  Hop\n",
      "Cell c_63 combined text (reversed):\n",
      "  Geboren Den 9 October 1775<br/>Moeder Cakleuna Ballert<br/>Jacob Beorg\n",
      "Cell c_63 combined text:\n",
      "  Geboren Den 9 October 1775<br/>Moeder Cakleuna Ballert<br/>Jacob Beorg\n",
      "Cell c_86 combined text (reversed):\n",
      "  Laatste Woonplaats Bruille<br/>beboortplaats Schoorkoven<br/>Geboren Den 9 October 1775<br/>Moeder Cakleuna Ballert\n",
      "Cell c_86 combined text:\n",
      "  Laatste Woonplaats Bruille<br/>beboortplaats Schoorkoven<br/>Geboren Den 9 October 1775<br/>Moeder Cakleuna Ballert\n",
      "Cell c_113 combined text (reversed):\n",
      "  \n",
      "Cell c_113 combined text:\n",
      "  \n",
      "Cell c_144 combined text (reversed):\n",
      "  \n",
      "Cell c_144 combined text:\n",
      "  \n",
      "Cell c_145 combined text (reversed):\n",
      "  \n",
      "Cell c_145 combined text:\n",
      "  \n",
      "Cell c_816 combined text (reversed):\n",
      "  Grest Herdrch<br/>van Schack\n",
      "Cell c_816 combined text:\n",
      "  Grest Herdrch<br/>van Schack\n",
      "Cell c_817 combined text (reversed):\n",
      "  Laatste Woonplaats ssage<br/>beboorkplaats Stip<br/>Geboren Den 23 december 1741<br/>Grest Herdrch\n",
      "Cell c_817 combined text:\n",
      "  Laatste Woonplaats ssage<br/>beboorkplaats Stip<br/>Geboren Den 23 december 1741<br/>Grest Herdrch\n",
      "Cell c_818 combined text (reversed):\n",
      "  Laatste Woonplaats ssage<br/>beboorkplaats Stip<br/>Geboren Den 23 december 1741<br/>Moeder Marin Om arbeek\n",
      "Cell c_818 combined text:\n",
      "  Laatste Woonplaats ssage<br/>beboorkplaats Stip<br/>Geboren Den 23 december 1741<br/>Moeder Marin Om arbeek\n",
      "Cell c_819 combined text (reversed):\n",
      "  \n",
      "Cell c_819 combined text:\n",
      "  \n",
      "Cell c_820 combined text (reversed):\n",
      "  \n",
      "Cell c_820 combined text:\n",
      "  \n",
      "Cell c_821 combined text (reversed):\n",
      "  \n",
      "Cell c_821 combined text:\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# -------- Overlap Mapping --------\n",
    "cell_text_map = {cid: [] for cid in cells}\n",
    "\n",
    "for tl in text_lines:\n",
    "    tl_poly = tl['polygon']\n",
    "    for cid, cell_poly in cells.items():\n",
    "        # Check if textline intersects or lies within cell polygon\n",
    "        if cell_poly.intersects(tl_poly):\n",
    "            cell_text_map[cid].append({\n",
    "                'textline_id': tl['id'],\n",
    "                'text': tl['text']\n",
    "            })\n",
    "\n",
    "cell_texts_combined = {}\n",
    "\n",
    "for cid, lines in cell_text_map.items():\n",
    "    # Extract only the text from each line in reverse order\n",
    "    texts = [line['text'] for line in reversed(lines) if line['text']]\n",
    "    combined_text = \"<br/>\".join(texts)\n",
    "    print(f\"Cell {cid} combined text (reversed):\\n  {combined_text}\")\n",
    "    \n",
    "    # Combine with <br/> if multiple lines\n",
    "    combined_text = \"<br/>\".join(texts)\n",
    "    \n",
    "    cell_texts_combined[cid] = combined_text\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Cell {cid} combined text:\\n  {combined_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84bfe008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def build_html_table_from_pagexml(xml_path, cell_texts_combined):\n",
    "    \"\"\"\n",
    "    Build an HTML <table> from a PAGE XML file and a mapping\n",
    "    of cell_id -> combined text.\n",
    "    \"\"\"\n",
    "    ns = {'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Collect all TableCells\n",
    "    cells_info = []\n",
    "    for cell in root.xpath('//ns:TableCell', namespaces=ns):\n",
    "        cell_id = cell.attrib['id']\n",
    "        row = int(cell.attrib.get('row', 0))\n",
    "        col = int(cell.attrib.get('col', 0))\n",
    "        rowspan = int(cell.attrib.get('rowSpan', 1))\n",
    "        colspan = int(cell.attrib.get('colSpan', 1))\n",
    "        text = cell_texts_combined.get(cell_id, '')\n",
    "\n",
    "        cells_info.append({\n",
    "            'id': cell_id,\n",
    "            'row': row,\n",
    "            'col': col,\n",
    "            'rowspan': rowspan,\n",
    "            'colspan': colspan,\n",
    "            'text': text\n",
    "        })\n",
    "\n",
    "    # Determine table size (max rows/columns)\n",
    "    max_row = max(c['row'] for c in cells_info)\n",
    "    max_col = max(c['col'] for c in cells_info)\n",
    "\n",
    "    # Build an empty grid\n",
    "    grid = [[None for _ in range(max_col+1)] for _ in range(max_row+1)]\n",
    "\n",
    "    # Place cells in grid\n",
    "    for c in cells_info:\n",
    "        grid[c['row']][c['col']] = c\n",
    "\n",
    "    # Generate HTML table string\n",
    "    html_lines = ['<table>']\n",
    "    for r in range(max_row+1):\n",
    "        html_lines.append('  <tr>')\n",
    "        for c in range(max_col+1):\n",
    "            cell = grid[r][c]\n",
    "            if cell:\n",
    "                td_attrs = f' id=\"{cell[\"id\"]}\"'\n",
    "                if cell['rowspan'] > 1:\n",
    "                    td_attrs += f' rowspan=\"{cell[\"rowspan\"]}\"'\n",
    "                if cell['colspan'] > 1:\n",
    "                    td_attrs += f' colspan=\"{cell[\"colspan\"]}\"'\n",
    "                html_lines.append(f'    <td{td_attrs}>{cell[\"text\"]}</td>')\n",
    "            else:\n",
    "                # empty cell\n",
    "                html_lines.append('    <td></td>')\n",
    "        html_lines.append('  </tr>')\n",
    "    html_lines.append('</table>')\n",
    "\n",
    "    return '\\n'.join(html_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdcb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML saved to data/tables/html/NL-HaNA_2.10.50_45_0355.jpg.html\n"
     ]
    }
   ],
   "source": [
    "directory = \"data/labels\"\n",
    "xml_path = os.path.join(directory, image_name)\n",
    "\n",
    "html_table = build_html_table_from_pagexml(xml_path, cell_texts_combined)\n",
    "\n",
    "# Get the folder of the XML file\n",
    "folder = \"data/tables/html/\"\n",
    "\n",
    "# Build the HTML file path (same name, .html extension)\n",
    "html_path = os.path.join(folder, image_name.replace('.xml', '.jpg') + \".html\")\n",
    "\n",
    "# Save the HTML\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_table)\n",
    "\n",
    "print(f\"HTML saved to {html_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64a9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Image2Table_LLM.parse import format_td\n",
    "from Image2Table_LLM.metric import TEDS\n",
    "def calculate_TEDS(ground_truth_html, predicted_html):\n",
    "    # predicted_html = format_td(predicted_html)\n",
    "    ground_truth_html = format_td(ground_truth_html)\n",
    "\n",
    "    teds = TEDS(structure_only=False)\n",
    "    teds_score = teds.evaluate(ground_truth_html, predicted_html)\n",
    "\n",
    "    teds_struct = TEDS(structure_only=True)\n",
    "    teds_struct_score = teds_struct.evaluate(ground_truth_html, predicted_html)\n",
    "    \n",
    "    print(f\"TEDS: {teds_score:.4f}\")\n",
    "    print(f\"TEDS-Struct: {teds_struct_score:.4f}\")\n",
    "\n",
    "    return teds_score, teds_struct_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dae4799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDS: 0.6343\n",
      "TEDS-Struct: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6343309061474394, 0.7857142857142857)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "with open(os.path.join(\"data/tables/html/\", image_name.replace('.xml', '.jpg')+'.html'), 'r', encoding='utf-8') as f:\n",
    "    constructed_html = f.read()\n",
    "\n",
    "with open(os.path.join(\"data/labels/\", image_name.replace('.xml', '.html')), 'r', encoding='utf-8') as f:\n",
    "    label_html = f.read()\n",
    "\n",
    "calculate_TEDS(label_html, constructed_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d1446",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(html_path):\n",
    "    if not os.path.exists(html_path):\n",
    "        raise FileNotFoundError(f\"HTML file not found: {html_path}\")\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        table = file.read()\n",
    "        return table\n",
    "    # print(f\"Successfully read HTML file: {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = read_html(f\"data/labels/{image_name}.html\")\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "table = soup.find(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45295a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "persons = []\n",
    "\n",
    "for row in table.find_all(\"tr\"):\n",
    "    cells = row.find_all(\"td\")\n",
    "\n",
    "    if not cells:\n",
    "        continue  # skip header or empty rows\n",
    "\n",
    "    person = {}\n",
    "    for cell in cells:\n",
    "        # preserve breaks as \\n\n",
    "        text = cell.get_text(separator=\"\\n\", strip=True)\n",
    "        \n",
    "        vader_match = re.search(r'Vader\\s+([^\\n]+)', text, re.IGNORECASE)\n",
    "        moeder_match = re.search(r'Moeder\\s+([^\\n]+)', text, re.IGNORECASE)\n",
    "        geboorte_datum_match = re.search(r'Geboren\\s*Den\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "        geboorte_plaats_match = re.search(r'Geboortplaats\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "        laatste_woonplaats_match = re.search(r'Laatste\\s*Woonplaats\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "        \n",
    "        if vader_match:\n",
    "            person['vader'] = {'value': vader_match.group(1).strip(), 'cell': cell.get('id')}\n",
    "        if moeder_match:\n",
    "            person['moeder'] = {'value': moeder_match.group(1).strip(), 'cell': cell.get('id')}\n",
    "        if geboorte_datum_match:\n",
    "            person['geboorte_datum'] = {'value': geboorte_datum_match.group(1).strip(), 'cell': cell.get('id')}\n",
    "        if geboorte_plaats_match:\n",
    "            person['geboorte_plaats'] = {'value': geboorte_plaats_match.group(1).strip(), 'cell': cell.get('id')}\n",
    "        if laatste_woonplaats_match:\n",
    "            person['laatste_woonplaats'] = {'value': laatste_woonplaats_match.group(1).strip(), 'cell': cell.get('id')}\n",
    "\n",
    "    if person:\n",
    "        persons.append(person)\n",
    "\n",
    "json_obj = {\"persons\": persons}\n",
    "print(json.dumps(json_obj, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d12d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/json/{image_name}.json\", \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(json_obj, json_file, ensure_ascii=False, indent=2)#!/usr/bin/env python3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687752a",
   "metadata": {},
   "source": [
    "### KG Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json from file\n",
    "with open(f\"data/json/{image_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_obj = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT ASSERSION TRIPLES\n",
    "from rdflib import Graph, ConjunctiveGraph, Namespace, URIRef, Literal, RDF\n",
    "\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "EX = Namespace(\"http://example.org/ontology/\")\n",
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "cg = ConjunctiveGraph()\n",
    "cg.bind(\"foaf\", FOAF)\n",
    "cg.bind(\"ex\", EX)\n",
    "cg.bind(\"prov\", PROV)\n",
    "\n",
    "# Mapping from json keys to RDF predicates\n",
    "predicate_map = {\n",
    "    \"vader\": EX.vader,\n",
    "    \"moeder\": EX.moeder,\n",
    "    \"geboorte_datum\": EX.geboorteDatum,\n",
    "    \"geboorte_plaats\": EX.geboortePlaats,\n",
    "    \"laatste_woonplaats\": EX.laatsteWoonplaats\n",
    "}\n",
    "\n",
    "for idx, person in enumerate(json_obj[\"persons\"], start=1):\n",
    "    person_uri = URIRef(f\"http://example.org/person/{idx}\")\n",
    "    assertion_graph_uri = URIRef(\"http://example.org/assertion\")\n",
    "    assertion_graph = Graph(store=cg.store, identifier=assertion_graph_uri)\n",
    "    assertion_graph.add((person_uri, RDF.type, FOAF.Person))\n",
    "\n",
    "    provenance_graph_uri = URIRef(\"http://example.org/provenance\")\n",
    "    provenance_graph = Graph(store=cg.store, identifier=provenance_graph_uri)\n",
    "\n",
    "    for key, value_dict in person.items():\n",
    "        value = value_dict[\"value\"]\n",
    "        cell_id = value_dict[\"cell\"]\n",
    "        predicate = predicate_map.get(key)\n",
    "        if predicate:\n",
    "            # Named graph for each cell\n",
    "            graph_uri = URIRef(f\"http://example.org/graph/{cell_id}\")\n",
    "            ng = Graph(store=cg.store, identifier=graph_uri)\n",
    "            ng.add((person_uri, predicate, Literal(value)))\n",
    "            \n",
    "\n",
    "cg.serialize(f\"data/triples/{image_name}_assersion.trig\", format='trig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee504691",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = \"2025-09-01T12:00:00Z\"  # Example end time, replace with actual time if needed\n",
    "start_time = \"2025-09-01T10:00:00Z\"  # Example start time, replace with actual time if needed\n",
    "\n",
    "# CONSTRUCT PROVENANCE TRIPLES\n",
    "from lxml import etree\n",
    "\n",
    "def add_provenance_graph(pagexml_path, stamboek_nummer=image_name):\n",
    "    tree = etree.parse(pagexml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    EX = Namespace(\"http://example.org/ontology/\")\n",
    "    IMG = Namespace(\"http://example.org/image_ontology/\")\n",
    "    RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")  \n",
    "    RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "    PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "    CSVW = Namespace(\"http://www.w3.org/ns/csvw#\")\n",
    "\n",
    "    # Create RDF graph\n",
    "    g = Graph()\n",
    "    g.bind(\"ex\", EX)\n",
    "    g.bind(\"img\", IMG)\n",
    "    g.bind(\"rdf\", RDF)\n",
    "    g.bind(\"rdfs\", RDFS)\n",
    "    g.bind(\"prov\", PROV)\n",
    "    g.csvw = (\"csvw\", CSVW)\n",
    "\n",
    "\n",
    "    # Find TableRegion(s)\n",
    "    table_regions = root.findall(\".//{*}TableRegion\")\n",
    "\n",
    "    for table_region in table_regions:\n",
    "        for cell in table_region.findall(\".//{*}TableCell\"):\n",
    "            cell_id = cell.get('id')\n",
    "            rows = cell.get('row')\n",
    "            cols = cell.get('col')\n",
    "            Coords = cell.find(\".//{*}Coords\")\n",
    "            coords_points = Coords.get('points') if Coords is not None else None\n",
    "            \n",
    "            # cell uri\n",
    "            named_graph_uri = URIRef(f\"http://example.org/graph/{cell_id}\")\n",
    "            cell_uri = URIRef(f\"http://example.org/id/{cell_id}\")\n",
    "            provenance_graph.add((named_graph_uri, PROV.wasDerivedFrom, cell_uri))\n",
    "            g.add((cell_uri, RDF.type, PROV.Entity))\n",
    "            g.add((cell_uri, RDFS.label, Literal(f\"Cell {cell_id} from {stamboek_nummer}\")))\n",
    "            \n",
    "            g.add((cell_uri, RDF.type, CSVW.Cell))\n",
    "            g.add((cell_uri, CSVW.rowNumber, Literal(rows)))\n",
    "            g.add((cell_uri, CSVW.columnNumber, Literal(cols)))\n",
    "            g.add((cell_uri, EX.ImageRegion, Literal(coords_points)))\n",
    "\n",
    "            # agents\n",
    "            agent_1 = URIRef(\"http://example.org/agent/1\")\n",
    "            g.add((agent_1, RDF.type, PROV.Agent))\n",
    "            g.add((agent_1, RDFS.label, Literal(\"Sarah Shoilee\")))\n",
    "            g.add((named_graph_uri, PROV.wasAttributedTo, agent_1))\n",
    "            project_agent = URIRef(\"http://example.org/agent/2\")\n",
    "            g.add((project_agent, RDF.type, PROV.Agent))\n",
    "            g.add((project_agent, RDFS.label, Literal(\"Pressing Matter Project\")))\n",
    "            g.add((agent_1, PROV.actedOnBehalfOf, project_agent))\n",
    "\n",
    "            # activity\n",
    "            stamboekenKGConstructionactivity = URIRef(f\"http://example.org/activity/stamboekenKGConstructionactivity/{cell_id}\")\n",
    "            tableConstructionactivity = URIRef(f\"http://example.org/activity/TableExtraction/{cell_id}\")\n",
    "            informationExtractionactivity = URIRef(f\"http://example.org/activity/InformationExtraction/{cell_id}\")\n",
    "            KGConstructionactivity = URIRef(f\"http://example.org/activity/KGConstruction/{cell_id}\")\n",
    "            \n",
    "            g.add((stamboekenKGConstructionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((named_graph_uri, PROV.wasGeneratedBy, stamboekenKGConstructionactivity))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasAssociatedWith, agent_1))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasInformedBy, tableConstructionactivity))\n",
    "            g.add((tableConstructionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasInformedBy, informationExtractionactivity))\n",
    "            g.add((informationExtractionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((informationExtractionactivity, PROV.used, cell_uri))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasInformedBy, KGConstructionactivity))\n",
    "            g.add((KGConstructionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((KGConstructionactivity, PROV.used, cell_uri))\n",
    "\n",
    "            g.add((stamboekenKGConstructionactivity,PROV.endedAtTime, Literal(end_time)))\n",
    "            g.add((stamboekenKGConstructionactivity,PROV.startedAtTime, Literal(start_time)))\n",
    "\n",
    "            # Create a Table instance URI\n",
    "            table_uri = URIRef(f\"http://example.org/Table/{cell_id}\")\n",
    "            g.add((table_uri, RDF.type, PROV.Entity))\n",
    "            g.add((table_uri, RDF.type, CSVW.Table))\n",
    "            g.add((table_uri, PROV.wasGeneratedBy, tableConstructionactivity))\n",
    "            g.add((cell_uri, PROV.wasDerivedFrom, table_uri))\n",
    "            \n",
    "            # stamboeken\n",
    "            stamboek_uri = URIRef(f\"http://example.org/stamboek/{stamboek_nummer}\")\n",
    "            g.add((stamboek_uri, RDF.type, PROV.Entity))\n",
    "            g.add((tableConstructionactivity, PROV.used, stamboek_uri))\n",
    "            g.add((table_uri, PROV.wasDerivedFrom, stamboek_uri))\n",
    "            national_archives = URIRef(\"http://example.org/agent/3\")\n",
    "            g.add((national_archives, RDF.type, PROV.Agent))\n",
    "            g.add((national_archives, RDFS.label, Literal(\"Nationaal Archief\")))\n",
    "            g.add((stamboek_uri, PROV.wasAttributedTo, national_archives))\n",
    "    \n",
    "    g.serialize(f\"data/triples/{image_name}_provenance.ttl\", format='ttl')\n",
    "add_provenance_graph(f\"data/labels/{image_name}.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htr_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
