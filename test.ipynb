{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json \n",
    "import xml.etree.ElementTree as ET\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/tables/pagexml\"\n",
    "image_name = \"NL-HaNA_2.10.50_45_0110.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668e6df",
   "metadata": {},
   "source": [
    "need to construct HTML Table form transkribus XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import pagexml_to_html\n",
    "\n",
    "pagexml_file = os.path.join(directory, image_name+ \".xml\")\n",
    "output_file = os.path.join(\"data/tables/html\", image_name + \".html\")\n",
    "pagexml_to_html(pagexml_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0a84a",
   "metadata": {},
   "source": [
    "### Measure mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922eef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import compute_mAP\n",
    "\n",
    "gt_file = \"data/labels/polygons/NL-HaNA_2.10.50_45_0091.jpg.polygons.json\"\n",
    "pred_file = \"data/tables/pagexml/NL-HaNA_2.10.50_45_0091.jpg.xml\"\n",
    "mAP = compute_mAP(gt_file, pred_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369840e",
   "metadata": {},
   "source": [
    "### Measure TED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import format_td\n",
    "from src.metrics import TEDS\n",
    "def calculate_TEDS(ground_truth_html, predicted_html):\n",
    "    # predicted_html = format_td(predicted_html)\n",
    "    ground_truth_html = format_td(ground_truth_html)\n",
    "\n",
    "    teds = TEDS(structure_only=False)\n",
    "    teds_score = teds.evaluate(ground_truth_html, predicted_html)\n",
    "\n",
    "    teds_struct = TEDS(structure_only=True)\n",
    "    teds_struct_score = teds_struct.evaluate(ground_truth_html, predicted_html)\n",
    "    \n",
    "    print(f\"TEDS: {teds_score:.4f}\")\n",
    "    print(f\"TEDS-Struct: {teds_struct_score:.4f}\")\n",
    "\n",
    "    return teds_score, teds_struct_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open(os.path.join(\"data/tables/html/\", image_name+ '.html'), 'r', encoding='utf-8') as f:\n",
    "    constructed_html = f.read()\n",
    "\n",
    "with open(os.path.join(\"data/labels/tables\", image_name+ '.html'), 'r', encoding='utf-8') as f:\n",
    "    label_html = f.read()\n",
    "\n",
    "calculate_TEDS(label_html, constructed_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d1446",
   "metadata": {},
   "source": [
    "### Information Extraction (using ONTOGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a139f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json \n",
    "\n",
    "image_name = \"NL-HaNA_2.10.50_45_0091.jpg\"\n",
    "with open(f\"data/tables/html{image_name}.html\", 'r', encoding='utf-8') as f:\n",
    "    constructed_html = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb2bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(constructed_html, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "# --- Step 1: make a grid keeping track of rowspans ---\n",
    "logical_rows = []\n",
    "rowspans = {}  # {(col_idx): [remaining_rows, cell_content]}\n",
    "\n",
    "for r_idx, tr in enumerate(rows):\n",
    "    # Start with any carried over rowspans from previous rows\n",
    "    current_row = []\n",
    "    to_remove = []\n",
    "\n",
    "    # Fill carried-down cells first\n",
    "    for col_idx, (remaining, cell) in rowspans.items():\n",
    "        current_row.append(cell)\n",
    "        rowspans[col_idx][0] -= 1\n",
    "        if rowspans[col_idx][0] <= 0:\n",
    "            to_remove.append(col_idx)\n",
    "    for col_idx in to_remove:\n",
    "        del rowspans[col_idx]\n",
    "\n",
    "    # Now add new cells from this row\n",
    "    c_idx = 0\n",
    "    for td in tr.find_all('td'):\n",
    "        while any(k == c_idx for k in rowspans):  # skip columns occupied by carried cells\n",
    "            c_idx += 1\n",
    "        text = td.get_text(\" \", strip=True)\n",
    "        cell_id = td.get('id')\n",
    "        r_idx = int(td.get('row', r_idx))\n",
    "        c_idx = int(td.get('col', c_idx))\n",
    "        rowspan = int(td.get('rowspan', 1))\n",
    "        colspan = int(td.get('colspan', 1))\n",
    "\n",
    "        cell_data = {\n",
    "            'text': text,\n",
    "            'id': cell_id,\n",
    "            'row': r_idx,\n",
    "            'col': c_idx,\n",
    "            'rowspan': rowspan,\n",
    "            'colspan': colspan\n",
    "        }\n",
    "\n",
    "        # Place cell in current row\n",
    "        current_row.append(cell_data)\n",
    "\n",
    "        # Store for future rows if rowspan > 1\n",
    "        if rowspan > 1:\n",
    "            rowspans[c_idx] = [rowspan - 1, cell_data]\n",
    "\n",
    "        c_idx += colspan\n",
    "\n",
    "    logical_rows.append(current_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8723ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1/3\n",
      "Running OntoGPT...\n",
      "\n",
      "Warning: YAML has no 'named_entities'. Using empty list.\n",
      "Processing row 2/3\n",
      "Running OntoGPT...\n",
      "\n",
      "Processing row 3/3\n",
      "Running OntoGPT...\n",
      "\n",
      "Warning: YAML has no 'named_entities'. Using empty list.\n"
     ]
    }
   ],
   "source": [
    "from src.person_info_extraction_ontogpt import extract_person_info as extractor\n",
    " \n",
    "\n",
    "temp_dir = \"data/temp/\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "for i, row in enumerate(logical_rows):\n",
    "    print(f\"Processing row {i+1}/{len(logical_rows)}\")\n",
    "    extracted_info = extractor(row, schema_path=\"data/schema/personbasicinfo.yaml\", json_output=f\"data/temp/person_{i}.json\",temp_dir=temp_dir, llm_model=\"ollama/llama3\")\n",
    "\n",
    "# concate json objects from differnt files and same it in a single file\n",
    "\n",
    "persons = []\n",
    "\n",
    "for filename in os.listdir(\"data/temp/\"):\n",
    "    if filename.endswith(\".json\") and filename.startswith(\"person_\"):\n",
    "        with open(os.path.join(\"data/temp/\", filename), 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # each temp file is expected to be {'persons': [...]}\n",
    "            person = data.get(\"persons\", [])\n",
    "            if isinstance(persons, list):\n",
    "                persons.extend(person)\n",
    "            elif person:\n",
    "                persons.append(persons)\n",
    "\n",
    "with open(f\"data/json/{image_name}.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump({\"persons\": persons}, f, indent=2, ensure_ascii=False)    \n",
    "\n",
    "# Step 6: Cleanup\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e1fcd",
   "metadata": {},
   "source": [
    "# Generate Information for ontoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fe5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "cell_spans = []     # list of dicts: {id, start, end}\n",
    "row_text = \"\"       # concatenated text of the entire table\n",
    "cursor = 0          # tracks current char offset\n",
    "\n",
    "for i, row in enumerate(logical_rows):\n",
    "    if i != 1:\n",
    "        continue\n",
    "    for cell in row:\n",
    "        text = cell[\"text\"]\n",
    "        cid = cell[\"id\"]\n",
    "\n",
    "        start = cursor\n",
    "        end = start + len(text)\n",
    "\n",
    "        cell_spans.append({\n",
    "            \"id\": cid,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "        row_text += text + '\\n'\n",
    "        cursor = end\n",
    "\n",
    "    with open(\"table_cells.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(cell_spans, f, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "    with open(\"row.txt\", \"w+\") as f:\n",
    "        f.write(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52210f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ontogpt extract -i row.txt -t personbasicinfo.yaml -m ollama/llama3 -o person.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bfb46",
   "metadata": {},
   "source": [
    "### map original_spans to cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def find_cell_for_span(start, spans):\n",
    "    \"\"\"\n",
    "    Return the cell_id whose text covers the character span starting at 'start'.\n",
    "    Span belongs to the cell where:  cell.start <= start < cell.end\n",
    "    \"\"\"\n",
    "    for item in spans:\n",
    "        if item[\"start\"] <= start < item[\"end\"]:\n",
    "            return item[\"id\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load ONTOGPT output file\n",
    "# ------------------------------\n",
    "\n",
    "yaml_path = \"person.yaml\"     \n",
    "with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "named_entities = data.get(\"named_entities\", [])\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load cell_spans index\n",
    "# ------------------------------\n",
    "\n",
    "yaml_path = \"table_cells.yaml\"     \n",
    "with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cell_spans = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3. For each named entity, map span to cell id\n",
    "# ------------------------------\n",
    "\n",
    "for ent in named_entities:\n",
    "    spans = ent.get(\"original_spans\", [])\n",
    "    ent_cells = []\n",
    "\n",
    "    for span_str in spans:\n",
    "        try:\n",
    "            start, end = map(int, span_str.split(\":\"))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        cell_id = find_cell_for_span(start, cell_spans)\n",
    "        ent_cells.append(cell_id)\n",
    "\n",
    "    # Add new slot \"cell\"\n",
    "    if ent_cells:\n",
    "        # if only one span, store a single value\n",
    "        ent[\"cell\"] = ent_cells[0] if len(ent_cells) == 1 else ent_cells\n",
    "    else:\n",
    "        ent[\"cell\"] = None\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Save updated YAML\n",
    "# ------------------------------\n",
    "\n",
    "output_path = \"person.yaml\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(data, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "print(\"✓ Updated YAML saved to\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfaf4c",
   "metadata": {},
   "source": [
    "### Convert YAML to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bf508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def load_yaml(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def get_entity_for_value(raw_value, named_entities):\n",
    "    \"\"\"\n",
    "    Given a raw value like 'AUTO:Wageningen',\n",
    "    return the full named_entity dict (label, cell, original_spans),\n",
    "    or None if not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_value, str):\n",
    "        return None\n",
    "\n",
    "    for ent in named_entities:\n",
    "        if ent.get(\"id\") == raw_value:\n",
    "            return ent\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_field(value, named_entities):\n",
    "    \"\"\"\n",
    "    Converts a YAML value into normalized JSON format with:\n",
    "    - value\n",
    "    - cell\n",
    "    - original_spans\n",
    "    Handles nested dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Case 1: nested dict → process each subfield recursively\n",
    "    if isinstance(value, dict):\n",
    "        processed = {}\n",
    "        for k, v in value.items():\n",
    "            processed[k] = process_field(v, named_entities)\n",
    "        return processed\n",
    "\n",
    "    # Case 2: literal field or AUTO reference\n",
    "    ent = get_entity_for_value(value, named_entities)\n",
    "\n",
    "    if ent:\n",
    "        # match found: use curated label + provenance\n",
    "        return {\n",
    "            \"value\": ent.get(\"label\"),\n",
    "            \"cell\": ent.get(\"cell\"),\n",
    "            \"original_spans\": ent.get(\"original_spans\")\n",
    "        }\n",
    "\n",
    "    # No entity match → raw literal value\n",
    "    return {\n",
    "        \"value\": value,\n",
    "        \"cell\": None,\n",
    "        \"original_spans\": None\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_yaml_to_person_json(data):\n",
    "    extracted = deepcopy(data[\"extracted_object\"])\n",
    "    named_entities = data[\"named_entities\"]\n",
    "\n",
    "    person = {}\n",
    "\n",
    "    # Process all fields dynamically\n",
    "    for key, value in extracted.items():\n",
    "        person[key] = process_field(value, named_entities)\n",
    "\n",
    "    return {\"persons\": [person]}\n",
    "\n",
    "\n",
    "def write_json(path, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    yaml_path = \"person.yaml\"\n",
    "    json_out = \"person.json\"\n",
    "\n",
    "    data = load_yaml(yaml_path)\n",
    "    result = convert_yaml_to_person_json(data)\n",
    "    write_json(json_out, result)\n",
    "\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90968651",
   "metadata": {},
   "source": [
    "# Information Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac971f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from src.person_info_extraction import extract_info_regex, extract_info_LLM\n",
    "\n",
    "persons = []\n",
    "\n",
    "for i, row in enumerate(logical_rows):\n",
    "    person = {}\n",
    "        \n",
    "    # print(generate_prompt(cells))\n",
    "    person = json.loads(extract_info_LLM(row))\n",
    "    if all(v['value']==None for v in person.values()):\n",
    "        continue\n",
    "\n",
    "    if person:\n",
    "        persons.append(person)\n",
    "\n",
    "unique_persons = {json.dumps(person, sort_keys=True) for person in persons}\n",
    "unique_persons_list = [json.loads(p) for p in unique_persons]\n",
    "        \n",
    "json_obj = {\"persons\": unique_persons_list}\n",
    "print(json.dumps(json_obj, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/json/{image_name}.json\", \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(json_obj, json_file, ensure_ascii=False, indent=2)#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8d4fe",
   "metadata": {},
   "source": [
    "### Calculate IE Precison * Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import best_match_similarity\n",
    "# from src.metrics import calculate_normalized_information_distance\n",
    "\n",
    "with open(f\"data/json/{image_name}.json\", 'r', encoding='utf-8') as f:\n",
    "    constructed_html = json.load(f)\n",
    "with open(os.path.join(\"data/labels/info\", image_name.replace('.jpg', '.json')), 'r', encoding='utf-8') as f:\n",
    "    label_html = json.load(f)\n",
    "\n",
    "# calculate_normalized_information_distance(constructed_html, label_html)\n",
    "print(best_match_similarity(label_html.get(\"persons\", []), constructed_html.get(\"persons\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d12526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "from src.metrics import infomration_extraction_precision_recall\n",
    "\n",
    "image_name = \"NL-HaNA_2.10.50_45_0091.jpg\"\n",
    "\n",
    "with open(f\"data/json/{image_name}.json\", 'r', encoding='utf-8') as f:\n",
    "    constructed_html = json.load(f)\n",
    "with open(os.path.join(\"data/labels/info\", image_name.replace('.jpg', '.json')), 'r', encoding='utf-8') as f:\n",
    "    label_html = json.load(f)\n",
    "\n",
    "# calculate_normalized_information_distance(constructed_html, label_html)\n",
    "print(infomration_extraction_precision_recall(label_html.get(\"persons\", []), constructed_html.get(\"persons\", []), threshold=0.4)) # distance threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687752a",
   "metadata": {},
   "source": [
    "# KG Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json from file\n",
    "with open(f\"data/json/{image_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_obj = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT ASSERSION TRIPLES\n",
    "from rdflib import Graph, ConjunctiveGraph, Namespace, URIRef, Literal, RDF\n",
    "\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "EX = Namespace(\"http://example.org/ontology/\")\n",
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "cg = ConjunctiveGraph()\n",
    "cg.bind(\"foaf\", FOAF)\n",
    "cg.bind(\"ex\", EX)\n",
    "cg.bind(\"prov\", PROV)\n",
    "\n",
    "# Mapping from json keys to RDF predicates\n",
    "predicate_map = {\n",
    "    \"vader\": EX.vader,\n",
    "    \"moeder\": EX.moeder,\n",
    "    \"geboorte_datum\": EX.geboorteDatum,\n",
    "    \"geboorte_plaats\": EX.geboortePlaats,\n",
    "    \"laatste_woonplaats\": EX.laatsteWoonplaats\n",
    "}\n",
    "\n",
    "for idx, person in enumerate(json_obj[\"persons\"], start=1):\n",
    "    person_uri = URIRef(f\"http://example.org/person/{idx}\")\n",
    "    assertion_graph_uri = URIRef(\"http://example.org/assertion\")\n",
    "    assertion_graph = Graph(store=cg.store, identifier=assertion_graph_uri)\n",
    "    assertion_graph.add((person_uri, RDF.type, FOAF.Person))\n",
    "\n",
    "    provenance_graph_uri = URIRef(\"http://example.org/provenance\")\n",
    "    provenance_graph = Graph(store=cg.store, identifier=provenance_graph_uri)\n",
    "\n",
    "    for key, value_dict in person.items():\n",
    "        value = value_dict[\"value\"]\n",
    "        cell_id = value_dict[\"cell\"]\n",
    "        predicate = predicate_map.get(key)\n",
    "        if predicate:\n",
    "            # Named graph for each cell\n",
    "            graph_uri = URIRef(f\"http://example.org/graph/{cell_id}\")\n",
    "            ng = Graph(store=cg.store, identifier=graph_uri)\n",
    "            ng.add((person_uri, predicate, Literal(value)))\n",
    "            \n",
    "\n",
    "cg.serialize(f\"data/triples/{image_name}_assersion.trig\", format='trig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee504691",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = \"2025-09-01T12:00:00Z\"  # Example end time, replace with actual time if needed\n",
    "start_time = \"2025-09-01T10:00:00Z\"  # Example start time, replace with actual time if needed\n",
    "\n",
    "# CONSTRUCT PROVENANCE TRIPLES\n",
    "from lxml import etree\n",
    "\n",
    "def add_provenance_graph(pagexml_path, stamboek_nummer=image_name):\n",
    "    tree = etree.parse(pagexml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    EX = Namespace(\"http://example.org/ontology/\")\n",
    "    IMG = Namespace(\"http://example.org/image_ontology/\")\n",
    "    RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")  \n",
    "    RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "    PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "    CSVW = Namespace(\"http://www.w3.org/ns/csvw#\")\n",
    "\n",
    "    # Create RDF graph\n",
    "    g = Graph()\n",
    "    g.bind(\"ex\", EX)\n",
    "    g.bind(\"img\", IMG)\n",
    "    g.bind(\"rdf\", RDF)\n",
    "    g.bind(\"rdfs\", RDFS)\n",
    "    g.bind(\"prov\", PROV)\n",
    "    g.csvw = (\"csvw\", CSVW)\n",
    "\n",
    "\n",
    "    # Find TableRegion(s)\n",
    "    table_regions = root.findall(\".//{*}TableRegion\")\n",
    "\n",
    "    for table_region in table_regions:\n",
    "        for cell in table_region.findall(\".//{*}TableCell\"):\n",
    "            cell_id = cell.get('id')\n",
    "            rows = cell.get('row')\n",
    "            cols = cell.get('col')\n",
    "            Coords = cell.find(\".//{*}Coords\")\n",
    "            coords_points = Coords.get('points') if Coords is not None else None\n",
    "            \n",
    "            # cell uri\n",
    "            named_graph_uri = URIRef(f\"http://example.org/graph/{cell_id}\")\n",
    "            cell_uri = URIRef(f\"http://example.org/id/{cell_id}\")\n",
    "            provenance_graph.add((named_graph_uri, PROV.wasDerivedFrom, cell_uri))\n",
    "            g.add((cell_uri, RDF.type, PROV.Entity))\n",
    "            g.add((cell_uri, RDFS.label, Literal(f\"Cell {cell_id} from {stamboek_nummer}\")))\n",
    "            \n",
    "            g.add((cell_uri, RDF.type, CSVW.Cell))\n",
    "            g.add((cell_uri, CSVW.rowNumber, Literal(rows)))\n",
    "            g.add((cell_uri, CSVW.columnNumber, Literal(cols)))\n",
    "            g.add((cell_uri, EX.ImageRegion, Literal(coords_points)))\n",
    "\n",
    "            # agents\n",
    "            agent_1 = URIRef(\"http://example.org/agent/1\")\n",
    "            g.add((agent_1, RDF.type, PROV.Agent))\n",
    "            g.add((agent_1, RDFS.label, Literal(\"Sarah Shoilee\")))\n",
    "            g.add((named_graph_uri, PROV.wasAttributedTo, agent_1))\n",
    "            project_agent = URIRef(\"http://example.org/agent/2\")\n",
    "            g.add((project_agent, RDF.type, PROV.Agent))\n",
    "            g.add((project_agent, RDFS.label, Literal(\"Pressing Matter Project\")))\n",
    "            g.add((agent_1, PROV.actedOnBehalfOf, project_agent))\n",
    "\n",
    "            # activity\n",
    "            stamboekenKGConstructionactivity = URIRef(f\"http://example.org/activity/stamboekenKGConstructionactivity/{cell_id}\")\n",
    "            tableConstructionactivity = URIRef(f\"http://example.org/activity/TableExtraction/{cell_id}\")\n",
    "            informationExtractionactivity = URIRef(f\"http://example.org/activity/InformationExtraction/{cell_id}\")\n",
    "            KGConstructionactivity = URIRef(f\"http://example.org/activity/KGConstruction/{cell_id}\")\n",
    "            \n",
    "            g.add((stamboekenKGConstructionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((named_graph_uri, PROV.wasGeneratedBy, stamboekenKGConstructionactivity))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasAssociatedWith, agent_1))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasInformedBy, tableConstructionactivity))\n",
    "            g.add((tableConstructionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasInformedBy, informationExtractionactivity))\n",
    "            g.add((informationExtractionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((informationExtractionactivity, PROV.used, cell_uri))\n",
    "            g.add((stamboekenKGConstructionactivity, PROV.wasInformedBy, KGConstructionactivity))\n",
    "            g.add((KGConstructionactivity, RDF.type, PROV.Activity))\n",
    "            g.add((KGConstructionactivity, PROV.used, cell_uri))\n",
    "\n",
    "            g.add((stamboekenKGConstructionactivity,PROV.endedAtTime, Literal(end_time)))\n",
    "            g.add((stamboekenKGConstructionactivity,PROV.startedAtTime, Literal(start_time)))\n",
    "\n",
    "            # Create a Table instance URI\n",
    "            table_uri = URIRef(f\"http://example.org/Table/{cell_id}\")\n",
    "            g.add((table_uri, RDF.type, PROV.Entity))\n",
    "            g.add((table_uri, RDF.type, CSVW.Table))\n",
    "            g.add((table_uri, PROV.wasGeneratedBy, tableConstructionactivity))\n",
    "            g.add((cell_uri, PROV.wasDerivedFrom, table_uri))\n",
    "            \n",
    "            # stamboeken\n",
    "            stamboek_uri = URIRef(f\"http://example.org/stamboek/{stamboek_nummer}\")\n",
    "            g.add((stamboek_uri, RDF.type, PROV.Entity))\n",
    "            g.add((tableConstructionactivity, PROV.used, stamboek_uri))\n",
    "            g.add((table_uri, PROV.wasDerivedFrom, stamboek_uri))\n",
    "            national_archives = URIRef(\"http://example.org/agent/3\")\n",
    "            g.add((national_archives, RDF.type, PROV.Agent))\n",
    "            g.add((national_archives, RDFS.label, Literal(\"Nationaal Archief\")))\n",
    "            g.add((stamboek_uri, PROV.wasAttributedTo, national_archives))\n",
    "    \n",
    "    g.serialize(f\"data/triples/{image_name}_provenance.ttl\", format='ttl')\n",
    "add_provenance_graph(f\"data/labels/{image_name}.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htr_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
