# Handwritten Table to Knowledge Graph

This project converts handwritten table images into structured Knowledge Graphs (KGs) with **triple-level provenance**.

❗ *Research Gap*: End-to-end processing of handwritten tables to structured Knowledge Graphs lacks reliability and traceability.

✅ Solution: We present a traceable and explainable pipeline that:

1. Reconstructs tables from scanned images (TSR + HTR)
2. Maps text content to source cells using bounding boxes or cell indexes.
3. Extracts RDF triples with **cell-level provenance**


---

## Pipeline Steps

### 1. Table Reconstruction (TSR + HTR)

📷 Convert image (e.g. jpg)→ structured table (e.g., HTML or 2D array)
```mermaid
graph TD;
   A((Image)) --> B[Cell Bounding Box Detection];
   A((Image)) --> C[Handwritten Text Recognition];
   B -- cell logical sequence <br/>cell bounding box coords --> D[Cell to Text Mapping]
   C --text lines with coords [pagexml]--> D;
   D --cell logical sequence + text content--> E[Construct Markup Table];
   E-->F((Table))

```

---

#### 2. Source-Aware Mapping

📍 Track table structure (bounding boxes or indexes) for each cell  
🔗 Supports triple-level provenance using Semantic Web / Linked Data models

---

#### 3. Information Extraction (IE) + KG Construction

📝 Extract values using:
- Regex patterns
- Large Language Models (LLMs)

🔄 Convert extracted information into RDF **triples**

# Set-up
#### 1. Install Mini-conda
Documentation: https://docs.anaconda.com/miniconda/

#### 2. Conda Create Environment

Create the environment from the environment.yml file:
```bash
conda env create -f environment.yml
```

#### 3. Loghi Instruction:
1. Pull loghi repo here 
```bash
git clone git@github.com:knaw-huc/loghi.git
```
2. Pull all the docker containers as per instructions
```bash
docker pull loghi/docker.laypa
docker pull loghi/docker.htr
docker pull loghi/docker.loghi-tooling
```
3. Go to: https://surfdrive.surf.nl/files/index.php/s/YA8HJuukIUKznSP and download 
   - a laypa model ("general") for detection of baselines and 
   - a loghi-htr model("float32-generic-2023-02-15") for HTR.
4. Specify the parameter paths accordingly (see original documentation of loghi)
5. Run command to run the loghi inference script:
```bash
scripts/inference-pipeline.sh ../images_samples/
```

# Scripts
## HTR
1. ```bash
   cd loghi
   ```
2. Change PATHS in [loghi/scripts/inference-pipeline.sh](loghi/scripts/inference-pipeline.sh), as specified in loghi module
3. type command
   ```bash
   scripts/inference-pipeline.sh ../images_samples/
   ```

## HTR on an entire stamboeken collections
```
python run_loghi.py
```

## Directory Structure

```
handwritten-table-kg/
│
├── 📁 data/                    # Raw and intermediate data files
│   ├── images/                 # Handwritten image files
│   ├── page/                   # HTR files generated by Loghi (PageXML)
│   ├── tables/                 # Structured table output (HTML, 2D arrays, CSV)
|      |── cells/               # output generated by LORE-TSR 
|      |── csv/                 # table in csv - cell logical secquence with text content
|      |── 2D/                  # table in 2D-array
|      |── html/                # table in html
│   ├── lines/                  # HTRed text lines with coords (csv files)
│   ├── triples/                # Final extracted RDF triples (TTL, JSON-LD, etc.)
│   └── examples/               # Example files for testing/demo
│
├── 📁 models/                  # Pretrained models
│   ├── laypa/                  # Layout analysis model
│   └── htr/                    # HTR model (e.g., float32-generic-2023-02-15)
│
├── 📁 src/                     # All source code
│   ├── __init__.py
│   ├── reconstruct_table.py   # Image → table reconstruction (TSR + HTR)
│   ├── generate_html.py       # Converts structured table to HTML
│   ├── extract_provenance.py  # Tracks source cells for triple-level provenance
│   ├── extract_triples.py     # Regex/LLM-based triple extraction
│   └── utils.py               # Common utility functions
│
├── 📁 scripts/                # Bash or python scripts for running pipeline
│   ├── run_pipeline.sh        # End-to-end runner (bash)
│   ├── run_loghi.py           # Loghi-based HTR runner
│   └── convert_to_triples.py  # Standalone triple generation
│
├── 📁 notebooks/              # Jupyter notebooks for experimentation
│   └── demo.ipynb
│
├── 📁 tests/                  # Unit and integration tests
│   └── test_extract_triples.py
│
├── 📄 environment.yml         # Conda environment file
├── 📄 requirements.txt        # (Optional) for pip environments
├── 📄 README.md               # Project description and usage
└── 📄 .gitignore              # Git ignore rules

```