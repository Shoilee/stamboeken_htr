# Handwritten Table to Knowledge Graph

This project converts handwritten table images into structured Knowledge Graphs (KGs) with **cell-level provenance**.

â— *Research Gap*: End-to-end processing of handwritten tables to structured Knowledge Graphs lacks reliability and traceability.

âœ… Solution: We present a traceable and explainable pipeline that:

1. Reconstructs tables from scanned images (TSR + HTR)
2. Maps text content to source cells using bounding boxes or cell indexes.
3. Extracts RDF triples with **cell-level provenance**

---
Pipeline Diagram
---

```pgsql
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                           IMAGE                               â”‚
 â”‚            (Handwritten Historical Table Page)                â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                 1. TABLE RECONSTRUCTION                       â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
 â”‚  â”‚ 1(A) Cell Detection (TSR)     â”‚   â”‚ 1(B)Handwriting (HTR)â”‚ â”‚
 â”‚  â”‚                               â”‚   â”‚ Recognizer           â”‚ â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
 â”‚            â”‚                                      â”‚           |
 â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Merge into PageXML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           |
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                  2. PAGE XML â†’ HTML TABLE                     â”‚
 â”‚  â€¢ Reconstruct structured <table><tr><td> from XML            â”‚
 â”‚    (preserving row/col spans + cell id)                       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚               3. ROW-LEVEL INFORMATION EXTRACTION             â”‚
 â”‚  For each table row:                                          â”‚
 â”‚   â€¢ Extract row text                                          â”‚
 â”‚   â€¢ OntoGPT(text, KG schema) â†’YAML                            â”‚
 â”‚   â€¢ YAML â†’ Normalized JSON                                    â”‚
 â”‚   â€¢ Add provenance (cell ID, text spans)                      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚               4. KNOWLEDGE GRAPH CONSTRUCTION                 â”‚
 â”‚   â€¢ Build assertion triples                                   â”‚
 â”‚   â€¢ Build provenance triples                                  â”‚
 â”‚   â€¢ Output as RDF (Trig)                                      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                     FINAL KNOWLEDGE GRAPH                     â”‚
 â”‚     (Traceable Entities to HTML Table & Source Image)         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---
## 1. Table Reconstruction (TSR + HTR)
### Approach-2 (Machine Learning Pipeline)

Convert image (e.g. jpg)â†’ structured table (e.g., HTML or 2D array)
```mermaid
graph TD;
   A((Image)) --> B[Cell Bounding Box Detection];
   A((Image)) --> C[Handwritten Text Recognition];
   B -- cell logical sequence <br/>cell bounding box coords --> D[Cell to Text Mapping]
   C --text lines with coords [pagexml]--> D;
   D --cell logical sequence + text content--> E[Construct Markup Table];
   E-->F((Table))

```
#### Cell Bounding Box Detection (using LORE-TSR)

- Add LORE-TSR as a Submodule

   ```bash
   git submodule add git@github.com:Shoilee/Image2TableBoundingBoxDetection.git
   cd Image2TableBoundingBoxDetection
   ```

   This repo uses a modified version of [LORE-TSR](https://github.com/AlibabaResearch/AdvancedLiterateMachineryDocumentUnderstanding/LORE-TSR)** designed to work on *CPU-only* systems (no CUDA required).

   > ğŸ’¡ *Note:* For full GPU (CUDA) support, use the original LORE-TSR repository.


- Create a conda environment:
   ```
   conda create --name Lore python=3.7
   conda activate Lore
   pip install -r requirements.txt
   ```

- Install DCNv2 from Scratch
   ```
   pip install Cython
   cd src/lib/models/networks/DCNv2
   chmod +x  *.sh
   ./make.sh
   ```

- Download Pretrained Model table cell detection model: model [ckpt_wireless](https://drive.google.com/file/d/1cBaewRwlZF1tIZovT49HpJZ5wlb3nSCw/view). Then create a model directory and move the downloaded file there:

   ```
   cd ../../../../../ 
   mkdir model
   ```

- Change the parameters such as model architecture, model path and input/output directory in `src/scripts/infer/demo_wired.sh`

- Run the scripts
   ```
   cd src
   bash scripts/infer/demo_wired.sh
   ```

---

#### Handwritten Text Recognition (using Loghi)

- Create the environment from the htr_env.yml file:
   ```bash
   conda env create -f htr_env.yml
   ```
- Pull loghi repo here 
   ```bash
   git clone git@github.com:knaw-huc/loghi.git
   ```
- Pull all the docker containers as per instructions
   ```bash
   docker pull loghi/docker.laypa
   docker pull loghi/docker.htr
   docker pull loghi/docker.loghi-tooling
   ```
- Go to: https://surfdrive.surf.nl/files/index.php/s/YA8HJuukIUKznSP and download 
   - a laypa model ("general") for detection of baselines and 
   - a loghi-htr model("float32-generic-2023-02-15") for HTR.

- Specify the parameter paths accordingly in `loghi/scripts/inference-pipeline.sh` (see original documentation of loghi)

- Run loghi to do HTR on table images
   ```
   python src/run_loghi.py
   ```


#### Reconstruct HTML table

   ```bash
   python src/reconstruct_table.py
   ```
---

### Approach-3 (Multi-run Conversation LLM)
1. 
```bash
cd Image2Table_LLM     
python run_modular_llm.py
```

2. 
```bash
run src/llm_pagexml.ipynb
```

---

## 2. Information Extraction (IE) 

ğŸ“ Extract attribute value, given desired schema, using:
- Regex patterns
- Large Language Models (LLMs)
- OntoGPT

## 3. KG Construction (Source-Aware Mapping)

ğŸ”„ Convert extracted information into RDF **triples**

ğŸ“ Track table structure (bounding boxes or indexes) for each cell  
ğŸ”— Supports triple-level provenance using Semantic Web / Linked Data models


## Directory Structure

```
handwritten-table-kg/
â”‚
â”œâ”€â”€ ğŸ“ data/                    # Raw and intermediate data files
â”‚   â”œâ”€â”€ images/                 # Handwritten image files
â”‚   â”œâ”€â”€ htr                     # HTR files generated by Loghi 
|      |â”€â”€ page/                # PageXML
|      |â”€â”€ csv/                 # text equivalent text per line 
â”‚   â”œâ”€â”€ tables/                 # Structured table output (HTML, 2D arrays, CSV)
|      |â”€â”€ cells/               # output generated by LORE-TSR 
|      |â”€â”€ csv/                 # table in csv - cell logical secquence with text content
|      |â”€â”€ 2D/                  # table in 2D-array
|      |â”€â”€ html/                # table in html
â”‚   â”œâ”€â”€ lines/                  # HTRed text lines with coords (csv files)
â”‚   â”œâ”€â”€ triples/                # Final extracted RDF triples (TTL, JSON-LD, etc.)
â”‚   â””â”€â”€ examples/               # Example files for testing/demo
â”‚
â”œâ”€â”€ ğŸ“ models/                  # Pretrained models
â”‚   â”œâ”€â”€ laypa/                  # Layout analysis model
â”‚   â””â”€â”€ htr/                    # HTR model (e.g., float32-generic-2023-02-15)
â”‚
â”œâ”€â”€ ğŸ“ src/                     # All source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reconstruct_table.py   # Image â†’ table reconstruction (TSR + HTR)
â”‚   â”œâ”€â”€ generate_html.py       # Converts structured table to HTML
â”‚   â”œâ”€â”€ extract_provenance.py  # Tracks source cells for triple-level provenance
â”‚   â”œâ”€â”€ extract_triples.py     # Regex/LLM-based triple extraction
â”‚   â””â”€â”€ utils.py               # Common utility functions
â”‚
â”œâ”€â”€ ğŸ“ scripts/                # Bash or python scripts for running pipeline
â”‚   â”œâ”€â”€ run_pipeline.sh        # End-to-end runner (bash)
â”‚   â”œâ”€â”€ run_loghi.py           # Loghi-based HTR runner
â”‚   â””â”€â”€ convert_to_triples.py  # Standalone triple generation
â”‚
â”œâ”€â”€ ğŸ“ notebooks/              # Jupyter notebooks for experimentation
â”‚   â””â”€â”€ demo.ipynb
â”‚
â”œâ”€â”€ ğŸ“ tests/                  # Unit and integration tests
â”‚   â””â”€â”€ test_extract_triples.py
â”‚
â”œâ”€â”€ ğŸ“„ environment.yml         # Conda environment file
â”œâ”€â”€ ğŸ“„ requirements.txt        # (Optional) for pip environments
â”œâ”€â”€ ğŸ“„ README.md               # Project description and usage
â””â”€â”€ ğŸ“„ .gitignore              # Git ignore rules

```