# Handwritten Table to Knowledge Graph Pipeline

A modular, traceable, and provenance-aware pipeline for converting handwritten historical tables into Knowledge Graphs.
This project converts handwritten table images into structured Knowledge Graphs (KGs) with **cell-level provenance**.

ğŸš€ Overview
---

Historical handwritten tables contain rich structured information but remain severely under-utilised due to challenges such as irregular layouts, degraded page conditions, and highly variable handwriting.
This repository provides a modular, traceble, provenance-aware pipeline to convert such archival tabular documents into Knowledge Graphs (KGs).

The system integrates:

- Table Structure Recognition (TSR)

- Handwritten Text Recognition (HTR)

- LLM-based information extraction

- Assersion Graph construction

- Provenance Graph construction, tracing from image â†’ cell â†’ entity

The pipeline supports multiple reconstruction approaches (Transkribus, ML-based detectors, modular LLM reconstruction) and captures human corrections for updated provenance and transparency.

---
Pipeline Diagram
---

![pipeline_diagram](pipeline_diagram.png)

---
## 1. Table Reconstruction (TSR + HTR)
### Approach-1 (Transkribus)
- Convert image to PageXML using the interface
- TODO: check if there is an API to do the job
### Approach-2 (Machine Learning Pipeline)

Convert image (e.g. jpg)â†’ structured table (e.g., HTML or 2D array)
```mermaid
graph TD;
   A((Image)) --> B["1(A) Cell Bounding Box Detection"];
   A((Image)) --> C["1(B) Handwritten Text Recognition"];
   B -- cell logical sequence <br/>cell bounding box coords --> D[2. Cell to Text Mapping]
   C --text lines with coords [pagexml]--> D;
   D --cell logical sequence + text content--> E[3. Construct Markup Table];
   E-->F((Table))

```
#### 1(A) Cell Bounding Box Detection (using LORE-TSR)

- Add LORE-TSR as a Submodule

   ```bash
   git submodule add git@github.com:Shoilee/Image2TableBoundingBoxDetection.git
   cd Image2TableBoundingBoxDetection
   ```

   This repo uses a modified version of [LORE-TSR](https://github.com/AlibabaResearch/AdvancedLiterateMachineryDocumentUnderstanding/LORE-TSR)** designed to work on *CPU-only* systems (no CUDA required).

   > ğŸ’¡ *Note:* For full GPU (CUDA) support, use the original LORE-TSR repository.


- Create a conda environment:
   ```
   conda create --name Lore python=3.7
   conda activate Lore
   pip install -r requirements.txt
   ```

- Install DCNv2 from Scratch
   ```
   pip install Cython
   cd src/lib/models/networks/DCNv2
   chmod +x  *.sh
   ./make.sh
   ```

- Download Pretrained Model table cell detection model: model [ckpt_wireless](https://drive.google.com/file/d/1cBaewRwlZF1tIZovT49HpJZ5wlb3nSCw/view). Then create a model directory and move the downloaded file there:

   ```
   cd ../../../../../ 
   mkdir model
   ```

- Change the parameters such as model architecture, model path and input/output directory in `src/scripts/infer/demo_wired.sh`

- Run the scripts
   ```
   cd src
   bash scripts/infer/demo_wired.sh
   ```

---

#### 1(B) Handwritten Text Recognition (using Loghi)

- Create the environment from the htr_env.yml file:
   ```bash
   conda env create -f htr_env.yml
   ```
- Pull loghi repo here 
   ```bash
   git clone git@github.com:knaw-huc/loghi.git
   ```
- Pull all the docker containers as per instructions
   ```bash
   docker pull loghi/docker.laypa
   docker pull loghi/docker.htr
   docker pull loghi/docker.loghi-tooling
   ```
- Go to: https://surfdrive.surf.nl/files/index.php/s/YA8HJuukIUKznSP and download 
   - a laypa model ("general") for detection of baselines and 
   - a loghi-htr model("float32-generic-2023-02-15") for HTR.

- Specify the parameter paths accordingly in `loghi/scripts/inference-pipeline.sh` (see original documentation of loghi)

- Run loghi to do HTR on table images
   ```
   python src/run_loghi.py
   ```


#### 3. Reconstruct HTML table

   ```bash
   python src/reconstruct_table.py
   ```
---

### Approach-3 (Multi-run Conversation LLM)
1. Run script to reconstruct table
```bash
cd Image2Table_LLM     
python run_modular_llm.py
```

2. Convert LLM output to pageXML
```bash
run src/llm_pagexml.ipynb
```

---

## 2. Information Extraction (IE) 

ğŸ“ Extract attribute value, given desired schema, using:
- Regex patterns
- Large Language Models (LLMs)
- OntoGPT

## 3. KG Construction (Source-Aware Mapping)

ğŸ”„ Convert extracted information into RDF **triples**

ğŸ“ Track table structure (bounding boxes or indexes) for each cell  
ğŸ”— Supports triple-level provenance using Semantic Web / Linked Data models


## Directory Structure

```
handwritten-table-kg/
â”‚
â”œâ”€â”€ ğŸ“ data/                    # Raw and intermediate data files
â”‚   â”œâ”€â”€ images/                 # Handwritten image files
â”‚   â”œâ”€â”€ htr                     # HTR files generated by Loghi 
|      |â”€â”€ page/                # PageXML
|      |â”€â”€ csv/                 # text equivalent text per line 
â”‚   â”œâ”€â”€ tables/                 # Structured table output (HTML, 2D arrays, CSV)
|      |â”€â”€ cells/               # output generated by LORE-TSR 
|      |â”€â”€ csv/                 # table in csv - cell logical secquence with text content
|      |â”€â”€ 2D/                  # table in 2D-array
|      |â”€â”€ html/                # table in html
â”‚   â”œâ”€â”€ lines/                  # HTRed text lines with coords (csv files)
â”‚   â”œâ”€â”€ triples/                # Final extracted RDF triples (TTL, JSON-LD, etc.)
â”‚   â””â”€â”€ examples/               # Example files for testing/demo
â”‚
â”œâ”€â”€ ğŸ“ models/                  # Pretrained models
â”‚   â”œâ”€â”€ laypa/                  # Layout analysis model
â”‚   â””â”€â”€ htr/                    # HTR model (e.g., float32-generic-2023-02-15)
â”‚
â”œâ”€â”€ ğŸ“ src/                     # All source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reconstruct_table.py   # Image â†’ table reconstruction (TSR + HTR)
â”‚   â”œâ”€â”€ generate_html.py       # Converts structured table to HTML
â”‚   â”œâ”€â”€ extract_provenance.py  # Tracks source cells for triple-level provenance
â”‚   â”œâ”€â”€ extract_triples.py     # Regex/LLM-based triple extraction
â”‚   â””â”€â”€ utils.py               # Common utility functions
â”‚
â”œâ”€â”€ ğŸ“ scripts/                # Bash or python scripts for running pipeline
â”‚   â”œâ”€â”€ run_pipeline.sh        # End-to-end runner (bash)
â”‚   â”œâ”€â”€ run_loghi.py           # Loghi-based HTR runner
â”‚   â””â”€â”€ convert_to_triples.py  # Standalone triple generation
â”‚
â”œâ”€â”€ ğŸ“ notebooks/              # Jupyter notebooks for experimentation
â”‚   â””â”€â”€ demo.ipynb
â”‚
â”œâ”€â”€ ğŸ“ tests/                  # Unit and integration tests
â”‚   â””â”€â”€ test_extract_triples.py
â”‚
â”œâ”€â”€ ğŸ“„ environment.yml         # Conda environment file
â”œâ”€â”€ ğŸ“„ requirements.txt        # (Optional) for pip environments
â”œâ”€â”€ ğŸ“„ README.md               # Project description and usage
â””â”€â”€ ğŸ“„ .gitignore              # Git ignore rules

```